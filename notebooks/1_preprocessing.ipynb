{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5d43f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from random import choice\n",
    "import torch\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Simple synonym dictionary for augmentation\n",
    "synonym_dict = {\n",
    "    'meeting': ['appointment', 'conference', 'session'],\n",
    "    'schedule': ['plan', 'arrange', 'organize'],\n",
    "    'issue': ['problem', 'concern', 'trouble'],\n",
    "    'thanks': ['thank', 'appreciate', 'gratitude'],\n",
    "    'update': ['progress', 'report', 'status']\n",
    "}\n",
    "\n",
    "def simple_augment(text):\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return text\n",
    "    words = text.split()\n",
    "    for i, word in enumerate(words):\n",
    "        if word in synonym_dict and np.random.rand() < 0.3:  # 30% chance to replace\n",
    "            words[i] = choice(synonym_dict[word])\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Load dataset (process in chunks for 500,000+ emails)\n",
    "chunk_size = 10000\n",
    "df_chunks = pd.read_csv('../data/emails.csv', chunksize=chunk_size)\n",
    "\n",
    "# Initialize lists to store processed data\n",
    "email_texts = []\n",
    "entities_list = []\n",
    "\n",
    "for chunk in df_chunks:\n",
    "    # Combine subject and message\n",
    "    chunk['email_text'] = chunk['file'].fillna('') + ' ' + chunk['message'].fillna('')\n",
    "    \n",
    "    # Advanced cleaning function with entity extraction\n",
    "    def clean_text(text):\n",
    "        if not isinstance(text, str):\n",
    "            return '', {}\n",
    "        text = re.sub(r'\\n+', ' ', text)  # Remove newlines\n",
    "        text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "        doc = nlp(text.lower())\n",
    "        tokens = [token.lemma_ for token in doc if not token.is_stop and token.is_alpha]\n",
    "        entities = {ent.label_: ent.text for ent in doc.ents if ent.label_ in [\"PERSON\", \"DATE\", \"TIME\"]}\n",
    "        return ' '.join(tokens), entities\n",
    "\n",
    "    chunk[['cleaned_text', 'entities']] = chunk['email_text'].apply(lambda x: pd.Series(clean_text(x)))\n",
    "    email_texts.extend(chunk['cleaned_text'].tolist())\n",
    "    entities_list.extend(chunk['entities'].tolist())\n",
    "    \n",
    "    break  # Process only first chunk\n",
    "\n",
    "# Zero-shot classification for labeling\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "intents = ['meeting_request', 'complaint', 'social', 'task_update', 'general']\n",
    "intent_descriptions = [\n",
    "    'Emails requesting to schedule or arrange a meeting or appointment.',\n",
    "    'Emails expressing dissatisfaction, issues, or complaints.',\n",
    "    'Emails with casual greetings, thanks, or social interactions.',\n",
    "    'Emails providing updates or progress on tasks or projects.',\n",
    "    'Emails with miscellaneous or unspecified content.'\n",
    "]\n",
    "\n",
    "# Encode intents and emails\n",
    "intent_embeddings = model.encode(intent_descriptions, convert_to_tensor=True)\n",
    "email_embeddings = model.encode(email_texts[:chunk_size], convert_to_tensor=True, show_progress_bar=True)\n",
    "\n",
    "# Compute cosine similarities\n",
    "similarities = torch.zeros((len(email_texts[:chunk_size]), len(intents)))\n",
    "for i in range(len(email_texts[:chunk_size])):\n",
    "    similarities[i] = torch.nn.functional.cosine_similarity(\n",
    "        email_embeddings[i].unsqueeze(0), intent_embeddings, dim=1\n",
    "    )\n",
    "labels = [intents[idx] for idx in similarities.argmax(dim=1).cpu().numpy()]\n",
    "\n",
    "# Create DataFrame\n",
    "df_sample = pd.DataFrame({\n",
    "    'email_text': email_texts[:chunk_size],\n",
    "    'label': labels,\n",
    "    'entities': entities_list[:chunk_size]\n",
    "}).reset_index(drop=True)  # Ensure unique index\n",
    "\n",
    "# Apply augmentation\n",
    "df_sample['augmented_text'] = df_sample['email_text'].apply(simple_augment)\n",
    "\n",
    "# Create augmented DataFrame and reset index\n",
    "df_augmented = df_sample[['augmented_text', 'label', 'entities']].rename(columns={'augmented_text': 'email_text'}).reset_index(drop=True)\n",
    "\n",
    "# Concatenate original and augmented DataFrames\n",
    "df_sample = pd.concat([df_sample[['email_text', 'label', 'entities']], df_augmented]).reset_index(drop=True)\n",
    "\n",
    "# Save labeled data\n",
    "df_sample = df_sample.sample(frac=1, random_state=42).reset_index(drop=True)  # Shuffle\n",
    "df_sample.to_csv('../data/labeled_emails.csv', index=False)\n",
    "\n",
    "print(\"Preprocessing Done! Labeled data saved with\", len(df_sample), \"samples.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
